{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Frequency Trading Algorithm\n",
    "\n",
    "You have been tasked by the investment firm Renaissance High Frequency Trading (RHFT) to develop an automated trading strategy utilizing a combination of machine learning algorithms and high frequency algorithms. RHFT wants this new algorithm to be based on stock market data of the 30 stocks in the Dow Jones at the minute level and to conduct buys and sells every minute based on 1 min, 5 min, and 10 min Momentum. The CIO asked you to choose the Machine Learning Algorithm best suited for this task and wants you to execute the trades via Alpaca's API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "\n",
    "api_key = 'PKKHA6KCMXSEQPSP9ZI9'\n",
    "secret_key = '4yLfOkgUrO898gbNKPcXEH0Gx0taRLDLv9TdvAUJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Alpaca API object, specifying use of the paper trading account:\n",
    "\n",
    "base_url = \"https://paper-api.alpaca.markets\"\n",
    "alpaca = tradeapi.REST(api_key, secret_key, base_url, api_version='v2')\n",
    "account = alpaca.get_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train and Compare Multiple Machine Learning Algorithms\n",
    "\n",
    " In this section, you'll train each of the requested algorithms and compare performance. Be sure to use the same parameters and training steps for each model. This is necessary to compare each model accurately.\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "#### 1. Generate your feature data (`X`) and target data (`y`):\n",
    "* Create a dataframe `X` that contains all the columns from the returns dataframe that will be used to predict `F_1_m_returns`.\n",
    "* Create a variable, called `y`, that is equal 1 if `F_1_m_returns` is larger than 0. This will be our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'returns.csv' does not exist: b'returns.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8edec5188476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the dataset returns.csv and set the index to level_0 and time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'returns.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'level_0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'level_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'returns.csv' does not exist: b'returns.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset returns.csv and set the index to level_0 and time\n",
    "\n",
    "returns = pd.read_csv('returns.csv', index_col = ['level_0', 'level_1'])\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe for features and define the target variable as a binary target\n",
    "X = returns.drop('F_1_m_returns', axis=1)\n",
    "\n",
    "# Create the target variable\n",
    "Y = returns['F_1_m_returns']\n",
    "Y = np.where(Y>0, 1, 0)\n",
    "Y = pd.Series(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "> Notice that we don't use shuffle when splitting the dataset into a training and testing dataset. \n",
    "\n",
    "> We want to keep the original ordering of the data, so we don't end up using observations in the future to predict past observations,\n",
    "\n",
    "> This is a critical mistake known as look ahead bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use the train_test_split library to split the dataset into a training and testing dataset, with 70% used for testing\n",
    "* Set the shuffle parameter to False, so that you use the first 70% for training to prvent look ahead bias.\n",
    "* Make sure you have these 4 variables: `X_train`, `X_test`, `y_train`, `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset without shuffling\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, random_state=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the `Counter` function to test the distribution of the data. \n",
    "* The result of `Counter({1: 668, 0: 1194})` reveals the data is indeed unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter function from the collections library\n",
    "from collections import Counter\n",
    "\n",
    "# Use Counter to count the number 1s and 0 in y_train\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balance the dataset with the Oversampler libary, setting `random state= 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomOverSampler from the imblearn library\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Use RandomOverSampler to resample the datase using random_state=1\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test the distribution once again with `Counter`. The new result of `Counter({1: 1194, 0: 1194})` shows the data is now balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Counter again to verify imbalance removed\n",
    "\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "#### 1. The first cells in this section provide an example of how to fit and train your model using the `LogisticRegression` model from sklearn:\n",
    "* Import select model.\n",
    "* Instantiate model object.\n",
    "* Fit the model to the resampled data - `X_resampled` and `y_resampled`.\n",
    "* Predict the model using `X_test`.\n",
    "* Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification_report from sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression model and train it on the X_resampled data we created before\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = log_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report toevaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use the same approach as above to train and test the following ML Algorithms:\n",
    "* [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "* [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "* [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier model and train it on the X_resampled data we created before\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = rfc_model.predict(X_test)\n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a GradientBoostingClassifier model and train it on the X_resampled data we created before\n",
    "gbc_model = GradientBoostingClassifier()\n",
    "gbc_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = gbc_model.predict(X_test)\n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create a AdaBoostClassifier model and train it on the X_resampled data we created before\n",
    "abc_model = AdaBoostClassifier()\n",
    "abc_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = gbc_model.predict(X_test)\n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a XGBClassifier model and train it on the X_resampled data we created before\n",
    "xgbc_model = XGBClassifier()\n",
    "xgbc_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = xgbc_model.predict(X_test)\n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using the classification report for each model, choose the model with the highest precision for use in your algo-trading program.\n",
    "#### 2. Save the selected model with the `joblib` libary to avoid retraining every time you wish to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the joblib library \n",
    "import joblib\n",
    "\n",
    "# Use the library to save the model that you want to use for trading\n",
    "joblib.dump(log_model, 'log_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement the strongest model using Apaca API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use the provided code to ping the Alpaca API and create the DataFrame needed to feed data into the model.\n",
    "   * This code will also store the correct feature data in `X` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tickers\n",
    "\n",
    "ticker_list = ['FB','AMZN','AAPL','NFLX', 'GOOGL', 'MSFT', 'TSLA']\n",
    "# Define Dates\n",
    "\n",
    "beg_date = '2021-01-06'\n",
    "end_date = '2021-01-06'\n",
    "\n",
    "# Convert the date in a format the Alpaca API reqires\n",
    "start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "end   =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=15, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "timeframe='1Min'\n",
    "\n",
    "# Use iloc to get the last 10 mins every time we pull new data\n",
    "prices = alpaca.get_barset(ticker_list, \"minute\", start=start, end=end).df.iloc[-11:]\n",
    "prices.ffill(inplace=True)   \n",
    "\n",
    "# Create an empty DataFrame for closing prices\n",
    "df_closing_prices = pd.DataFrame()\n",
    "\n",
    "# Fetch the closing prices of our tickers\n",
    "df_closing_prices[\"FB\"] = prices[\"FB\"][\"close\"]\n",
    "df_closing_prices[\"AMZN\"] = prices[\"AMZN\"][\"close\"]\n",
    "df_closing_prices[\"AAPL\"] = prices[\"AAPL\"][\"close\"]\n",
    "df_closing_prices[\"NFLX\"] = prices[\"NFLX\"][\"close\"]\n",
    "df_closing_prices[\"GOOGL\"] = prices[\"GOOGL\"][\"close\"]\n",
    "df_closing_prices['MSFT'] = prices['MSFT'][\"close\"]\n",
    "df_closing_prices['TSLA'] = prices['TSLA'][\"close\"]\n",
    "\n",
    "print(df_closing_prices.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of momentums\n",
    "list_of_momentums = [1,5,10]\n",
    "\n",
    "for i in list_of_momentums:  \n",
    "    # Compute percentage change for each one of the momentums in the momentum list\n",
    "    returns_temp = df_closing_prices.pct_change(i)\n",
    "    # Unstack the returns \n",
    "    returns_temp = pd.DataFrame(returns_temp.unstack())\n",
    "    name = f'{i}_m_returns'\n",
    "    returns_temp.rename(columns={0: name}, inplace = True)\n",
    "    # Reset the index so we can merge based on index\n",
    "    returns_temp.reset_index(inplace = True)\n",
    "    # Merge newly computed returns with previously created returns\n",
    "    if i ==1:\n",
    "        returns = returns_temp\n",
    "    else:\n",
    "        returns = pd.merge(returns,returns_temp,left_on=['level_0', 'time'],right_on=['level_0', 'time'], how='left', suffixes=('_original', 'right'))\n",
    "\n",
    "# Drop nulls and set index\n",
    "returns.dropna(axis=0, how='any', inplace=True)\n",
    "returns.set_index(['level_0', 'time'], inplace=True)\n",
    "\n",
    "# Generate feature data and preview first 10 rows.\n",
    "X = returns\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using `joblib`, load the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously trained and saved model using joblib\n",
    "\n",
    "final_model = joblib.load('log_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the model file to make predicttions:\n",
    "* Use `predict` on `X` and save this as `y_pred`.\n",
    "* Convert `y_pred` to a DataFrame, setting the index to the index of `X`.\n",
    "* Rename the column 0 to 'buy', be sure to set `inplace =True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model file to predict on X\n",
    "y_pred = final_model.predict(X)\n",
    "\n",
    "# Convert y_pred to a dataframe, set the index to the index of X\n",
    "y_pred_df = pd.DataFrame(y_pred, index=X.index)\n",
    "\n",
    "# Rename the column 0 to 'buy', be sure to set inplace =True\n",
    "y_pred_df.rename(columns={0:'buy'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter the stocks where 'buy' is equal to 1, saving the filter as `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the stocks where 'buy' is equal to 1\n",
    "\n",
    "buy_df = y_pred_df[y_pred_df['buy'] == 1]\n",
    "buy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Using the `y_pred` filter, create a dictionary called `buy_dict` and assign 'n' to each Ticker (key value) as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary from y_pred and assign a 'n' to each of them for now as a placeholder.\n",
    "buy_dict = dict.fromkeys(y_pred_df.index.get_level_values(0), 'n')\n",
    "buy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Obtain the total available equity in your account from the Alpaca API and store in a variable called `total_capital`. You will split the capital equally between all selected stocks per the CIO's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the total available equity in our account from the  Alpaca API\n",
    "\n",
    "total_capital = int(account.equity)\n",
    "total_capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute capital per stock, divide equity in account by number of stocks\n",
    "# Use Alpaca API to pull the equity in the account\n",
    "if len(buy_dict) > 0:\n",
    "    capital_per_stock = float(total_capital)/ len(buy_dict)\n",
    "else:\n",
    "    capital_per_stock = 0\n",
    "print(f'Capital per stock: {capital_per_stock}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Use a for-loop to iterate through `buy_dict` to determine the number stocks you need to buy for each ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for loop to iterate through dictionary of buys \n",
    "# Determine the number stocks we need to buy for each ticker\n",
    "for ticker in buy_dict:\n",
    "    try:\n",
    "        buy_dict[ticker] = int(capital_per_stock /int(prices[ticker].iloc[-1]['close']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(buy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Cancel all previous orders in the Alpaca API (so you don't buy more than intended) and sell all currently held stocks to close all positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel all previous orders in the Alpaca API\n",
    "alpaca.cancel_all_orders()\n",
    "\n",
    "# Sell all currently held stocks to close all positions\n",
    "alpaca.close_all_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Iterate through `buy_dict` and send a buy order for each ticker with their corresponding number of shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the longlist object and send a buy order for each ticker with a corresponding number of shares:\n",
    "\n",
    "for key,value in buy_dict.items():\n",
    "    alpaca.submit_order(symbol=key, qty=value, side=\"buy\", type=\"market\", time_in_force=\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca.cancel_all_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Make a function called `trade()` that incorporates all of the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all of the steps conducted above into the function trade\n",
    "def trade():\n",
    "\n",
    "    ticker_list = ['FB','AMZN','AAPL','NFLX', 'GOOGL', 'MSFT', 'TSLA']\n",
    "    # Notice that we remove the start and end variables since we want the latest prices.\n",
    "    timeframe='1Min'\n",
    "    # Use iloc to get the last 10 mins every time we pull new data\n",
    "    prices = alpaca.get_barset(ticker_list, \"minute\").df.iloc[-11:]\n",
    "    prices.ffill(inplace=True)   \n",
    "\n",
    "    # Create and empty DataFrame for closing prices\n",
    "    df_closing_prices = pd.DataFrame()\n",
    "\n",
    "    # Fetch the closing prices of our tickers\n",
    "    df_closing_prices[\"FB\"] = prices[\"FB\"][\"close\"]\n",
    "    df_closing_prices[\"AMZN\"] = prices[\"AMZN\"][\"close\"]\n",
    "    df_closing_prices[\"AAPL\"] = prices[\"AAPL\"][\"close\"]\n",
    "    df_closing_prices[\"NFLX\"] = prices[\"NFLX\"][\"close\"]\n",
    "    df_closing_prices[\"GOOGL\"] = prices[\"GOOGL\"][\"close\"]\n",
    "    df_closing_prices['MSFT'] = prices['MSFT'][\"close\"]\n",
    "    df_closing_prices['TSLA'] = prices['TSLA'][\"close\"]\n",
    "    print(df_closing_prices.head())\n",
    "    \n",
    "    # Loop through momentums to build new DataFrame\n",
    "    list_of_momentums = [1,5,10]\n",
    "    for i in list_of_momentums:   \n",
    "        returns_temp = df_closing_prices.pct_change(i)\n",
    "        returns_temp = pd.DataFrame(returns_temp.unstack())\n",
    "        name = f'{i}_m_returns'\n",
    "        returns_temp.rename(columns={0: name}, inplace = True)\n",
    "        returns_temp.reset_index(inplace = True)\n",
    "        if i ==1:\n",
    "            returns = returns_temp\n",
    "        else:\n",
    "            returns = pd.merge(returns,returns_temp,left_on=['level_0', 'time'],right_on=['level_0', 'time'], how='left', suffixes=('_original', 'right'))\n",
    "\n",
    "    # Drop nulls and set index            \n",
    "    returns.dropna(axis=0, how='any', inplace=True)\n",
    "    returns.set_index(['level_0', 'time'], inplace=True)\n",
    "    \n",
    "    # Preprocess data for model\n",
    "    X = returns\n",
    "    final_model = joblib.load('log_model.pkl')\n",
    "    y_pred = final_model.predict(X)\n",
    "    y_pred_df = pd.DataFrame(y_pred, index=X.index)\n",
    "    y_pred_df.rename(columns={0:'buy'}, inplace=True)\n",
    "    buy_df = y_pred_df[y_pred_df['buy'] == 1]\n",
    "\n",
    "    # Create the `buy_dict` object\n",
    "    buy_dict = dict.fromkeys(y_pred_df.index.get_level_values(0), 'n')\n",
    "    \n",
    "    # Split capital between stocks and determine buy or sell\n",
    "    total_capital = int(account.equity)\n",
    "    if len(buy_dict) > 0:\n",
    "        capital_per_stock = float(total_capital)/ len(buy_dict)\n",
    "    else:\n",
    "        capital_per_stock = 0\n",
    "    for ticker in buy_dict:\n",
    "        try:\n",
    "            buy_dict[ticker] = int(capital_per_stock /int(prices[ticker].iloc[-1]['close']))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Cancel pending orders and close positions\n",
    "    alpaca.cancel_all_orders()\n",
    "    alpaca.close_all_positions()\n",
    "   \n",
    "    \n",
    "    # Submit orders\n",
    "    for key,value in buy_dict.items():\n",
    "        alpaca.submit_order(symbol=key, qty=value, side=\"buy\", type=\"market\", time_in_force=\"day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import Python's schedule module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python's schedule module \n",
    "import schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the \"schedule\" module to automate the algorithm:\n",
    "* Clear the schedule with `.clear()`.\n",
    "* Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. `10:31:05`).\n",
    "* Use the Alpaca API to check whether the market is open.\n",
    "* Use run_pending() function inside schedule to execute the schedule you defined while the market is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the schedule\n",
    "schedule.clear()\n",
    "\n",
    "# Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. 10:31:05)\n",
    "schedule.every().minute.at(\":05\").do(trade)\n",
    "\n",
    "# Use the Alpaca API to check whether the market is open\n",
    "clock = alpaca.get_clock()\n",
    "\n",
    "# Use run_pending() function inside schedule to execute the schedule you defined as long as the market is open\n",
    "while clock.is_open:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvizenv] *",
   "language": "python",
   "name": "conda-env-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
